in this topic are tips and suggestions about process design and development, data migration and improving performance. design and development when designing a process, ask yourself these initial questions: what does the source data look like and what does the destination data need to look like in terms of fields, format, structure and delimiters? what mapping, transformations, lookups, etc. need to be performed to get the data from source to destination? make sure the source and destination data profiles are configured to what the data should look like with respect to data types, value formats (e.g., dates), delimiters, etc. use map functions to transform the values. when mapping a date field to another date field, configure the source and destination profile elements with the respective date masks and let the integration mapping engine reformat the values implicitly, rather than using a date format map function. think sequentially! start simple and iterate. create a very simple process that moves data from source to destination to make sure the connections, filters and basic mapping are correct. then begin introducing validations, routing, advanced mapping/functions, custom error handling, logging, etc. start with the end points. when building a new process from scratch, begin by configuring the end points: the source connection/operation in the start shape and the destination connection/operation in a connector shape. the operation configuration (for most connectors) will automatically generate the profile components that you will use in the map shapes, decision shapes or other shapes. know what the document data looks like before and after every step. you can use test mode to review the source data at every step in the process. use decision and/or route shapes to address common business validation scenarios or ones that require a different workflow. do not try to replicate all the business logic that occurs in the destination application. try to avoid passing no documents from one step to the next step. when documents move from step to step, tracking information is passed along with them. in most cases, a step expects to receive at least one document and tracking information from the previous step. if a step doesn’t receive documents or tracking information, an exception error may be generated. try to create common, reusable components, especially for things like map functions. create user-defined map functions for all but the simplest mapping transformations to easily reuse and reconfigure the function. use dynamic document properties to easily track arbitrary values for a given document without having to stage data somewhere or include it in the document's actual data. (often this is a precluded by the connector's profile.) consider where potential points of failure are and how you will capture and handle errors. ask yourself, "how often does a process really need to run?" few integrations actually need to be truly real-time, event-based or need to run every minute. try to schedule processes every 15-30 minutes, hourly or daily. this will make development, testing and, more importantly, monitoring easier. extract records incrementally. avoid extracting all records every time. most systems capture the last modified date per record. use connector operation filters to select only records that have been modified since the last time the process ran. (the preferred method is to query by the process's last successful run date or use your own persisted process property to capture the last modified date from the records themselves.) see the process property components topic linked below for more information. another approach is to extract records based on some status or flag field and then update that field to another value after the records have been processed successfully. this can be a good option if your application allows custom or user-defined fields because it is easy for end users to revert the status to reprocess a record. limit the amount of records. do not select all 100,000 records from your database the first time that you run the process. during initial testing, mock up small test files or use filters to restrict the number of records being processed (e.g., select specific records by id or name). as you get more comfortable with the process, then introduce larger data sets. data migration here are some tips to follow when you are migrating data. export or import data to intermediate csv files. this will enable you to easily massage, sort and "de-dupe" records as necessary and it will facilitate reprocessing. begin by importing a small set of records to verify your process, mapping, validations, etc. slowly increase the number of records. give consideration to how invalid records and errors will be reported. use connector operation response handling to help identify which records failed. consider appending warnings to an arbitrary local log file. use the notify shape to write custom log messages to a local log file to get processing results in real time. this is preferable to waiting for results to be reported on the manage menu's process reporting page at the completion of the process. improving performance here are some suggestions to follow to improve performance: understand how using cached values in map function components and decision shapes can improve performance. stagger process schedules to maintain performance levels. many of our users schedule their processes to start at the top of the hour. if possible, make your processes start at other times. set the allow simultaneous executions check box in the process options dialog to allow or prevent repeated executions of the process within a certain time interval. limit the number of connections and connector calls. look for opportunities to batch queries and requests. note that many application connections batch requests automatically for you "behind the scenes". if you need to route documents based on multiple values from a lookup, do one lookup in a map function and capture the results in dynamic document properties rather than doing the same connector call in multiple decision and/or route shapes. see the document properties topic linked below for more information. use cross reference table components for small, static sets of translation values rather than making connector calls. use the process call shape to call sub-processes. if you clear its “wait for process to complete” check box, it will allow you to run more different tasks at the same time. you can use the flow control shape to run more of the same tasks at the same time. however, the run each document individually (“for each”) option within the flow control shape slows down any process, including processes whose mode is set to general rather than to low latency. use this option sparingly. consider using the combine and split documents processing steps in the data process shape to group common data sets. this is better than sending these documents individually into logic steps that perform lookups or connector calls. note that setting the last successful run date parameter value in the start shape in a new process will extract all records from the source application because the deployed process has never before been executed. if appropriate, consider extracting all the records from the destination application and storing them locally (e.g., in a database table) to perform quicker lookups. this is often helpful when integrating or migrating very large sets of data to/from an online application, because remote web service calls are much more time consuming than local database calls. see the troubleshooting topic linked below to review answers to common questions about memory inefficiencies in the atom.